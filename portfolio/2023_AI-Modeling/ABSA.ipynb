{"cells":[{"cell_type":"markdown","metadata":{"id":"Act-kLyB_S4Z"},"source":["# 데이터 로드 및 파싱"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":73787,"status":"ok","timestamp":1703997670691,"user":{"displayName":"Trip Builder","userId":"18334374602196460472"},"user_tz":-540},"id":"9ueKkXHfFRC2","outputId":"40fffdbf-5a06-4356-c957-62d8c4188c31"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/haven-jeon/PyKoSpacing.git\n","  Cloning https://github.com/haven-jeon/PyKoSpacing.git to /tmp/pip-req-build-vkeif6at\n","  Running command git clone --filter=blob:none --quiet https://github.com/haven-jeon/PyKoSpacing.git /tmp/pip-req-build-vkeif6at\n","  Resolved https://github.com/haven-jeon/PyKoSpacing.git to commit 04aeebcbe26b109486a642e57dc58665c4818cf3\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting tensorflow==2.11.1 (from pykospacing==0.5)\n","  Downloading tensorflow-2.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting h5py==3.10.0 (from pykospacing==0.5)\n","  Downloading h5py-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting argparse>=1.4.0 (from pykospacing==0.5)\n","  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from h5py==3.10.0->pykospacing==0.5) (1.23.5)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.1->pykospacing==0.5) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.1->pykospacing==0.5) (1.6.3)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.1->pykospacing==0.5) (23.5.26)\n","Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.11.1->pykospacing==0.5)\n","  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.1->pykospacing==0.5) (0.2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.1->pykospacing==0.5) (1.60.0)\n","Collecting keras<2.12,>=2.11.0 (from tensorflow==2.11.1->pykospacing==0.5)\n","  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.1->pykospacing==0.5) (16.0.6)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.1->pykospacing==0.5) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.1->pykospacing==0.5) (23.2)\n","Collecting protobuf<3.20,>=3.9.2 (from tensorflow==2.11.1->pykospacing==0.5)\n","  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.1->pykospacing==0.5) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.1->pykospacing==0.5) (1.16.0)\n","Collecting tensorboard<2.12,>=2.11 (from tensorflow==2.11.1->pykospacing==0.5)\n","  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorflow-estimator<2.12,>=2.11.0 (from tensorflow==2.11.1->pykospacing==0.5)\n","  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 kB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.1->pykospacing==0.5) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.1->pykospacing==0.5) (4.5.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.1->pykospacing==0.5) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.1->pykospacing==0.5) (0.34.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.11.1->pykospacing==0.5) (0.42.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.1->pykospacing==0.5) (2.17.3)\n","Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.12,>=2.11->tensorflow==2.11.1->pykospacing==0.5)\n","  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.1->pykospacing==0.5) (3.5.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.1->pykospacing==0.5) (2.31.0)\n","Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.12,>=2.11->tensorflow==2.11.1->pykospacing==0.5)\n","  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.12,>=2.11->tensorflow==2.11.1->pykospacing==0.5)\n","  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.1->pykospacing==0.5) (3.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.1->pykospacing==0.5) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.1->pykospacing==0.5) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.1->pykospacing==0.5) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.1->pykospacing==0.5) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.1->pykospacing==0.5) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.1->pykospacing==0.5) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.1->pykospacing==0.5) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.1->pykospacing==0.5) (2023.11.17)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow==2.11.1->pykospacing==0.5) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.1->pykospacing==0.5) (0.5.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.1->pykospacing==0.5) (3.2.2)\n","Building wheels for collected packages: pykospacing\n","  Building wheel for pykospacing (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pykospacing: filename=pykospacing-0.5-py3-none-any.whl size=2270663 sha256=627e06f3f9caa6dde9330ac7a9e94f17a76ba185e10d2af69ac3d2de8920a5a2\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-2655jtn7/wheels/76/b3/33/dda14886ee76b8e53eb05580a14dfcf9145e9eb9d282c53f28\n","Successfully built pykospacing\n","Installing collected packages: tensorboard-plugin-wit, argparse, tensorflow-estimator, tensorboard-data-server, protobuf, keras, h5py, gast, google-auth-oauthlib, tensorboard, tensorflow, pykospacing\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.15.0\n","    Uninstalling tensorflow-estimator-2.15.0:\n","      Successfully uninstalled tensorflow-estimator-2.15.0\n","  Attempting uninstall: tensorboard-data-server\n","    Found existing installation: tensorboard-data-server 0.7.2\n","    Uninstalling tensorboard-data-server-0.7.2:\n","      Successfully uninstalled tensorboard-data-server-0.7.2\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 3.20.3\n","    Uninstalling protobuf-3.20.3:\n","      Successfully uninstalled protobuf-3.20.3\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.15.0\n","    Uninstalling keras-2.15.0:\n","      Successfully uninstalled keras-2.15.0\n","  Attempting uninstall: h5py\n","    Found existing installation: h5py 3.9.0\n","    Uninstalling h5py-3.9.0:\n","      Successfully uninstalled h5py-3.9.0\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.5.4\n","    Uninstalling gast-0.5.4:\n","      Successfully uninstalled gast-0.5.4\n","  Attempting uninstall: google-auth-oauthlib\n","    Found existing installation: google-auth-oauthlib 1.2.0\n","    Uninstalling google-auth-oauthlib-1.2.0:\n","      Successfully uninstalled google-auth-oauthlib-1.2.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.15.1\n","    Uninstalling tensorboard-2.15.1:\n","      Successfully uninstalled tensorboard-2.15.1\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.15.0\n","    Uninstalling tensorflow-2.15.0:\n","      Successfully uninstalled tensorflow-2.15.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","pandas-gbq 0.19.2 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n","tensorflow-datasets 4.9.4 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n","tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed argparse-1.4.0 gast-0.4.0 google-auth-oauthlib-0.4.6 h5py-3.10.0 keras-2.11.0 protobuf-3.19.6 pykospacing-0.5 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.1 tensorflow-estimator-2.11.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["argparse","google"]}}},"metadata":{}}],"source":["!pip install git+https://github.com/haven-jeon/PyKoSpacing.git"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":11297,"status":"ok","timestamp":1703997681984,"user":{"displayName":"Trip Builder","userId":"18334374602196460472"},"user_tz":-540},"id":"UJCoHZqV_SkM"},"outputs":[],"source":["import re\n","import json\n","import numpy as np\n","import pandas as pd\n","\n","from pykospacing import Spacing\n","\n","import torch\n","from torch.utils.data import  TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from transformers import BertTokenizer"]},{"cell_type":"markdown","metadata":{"id":"AQfYv8UnWgpo"},"source":["# 전처리"]},{"cell_type":"markdown","metadata":{"id":"lihSJNGiposw"},"source":["- 한 행에 한 문장 들어가도록\n","\n","- clean_text: 텍스트 정제 함수\n","- InputExample: 텍스트, 라벨 연결해주는 클래스\n","- SequenceFeatures: 임베딩 시킨 피쳐들을 연결해주는 클래스\n","- convert_examples_to_seq_features: 문장을 임베딩한 피쳐로 변환하는 함수\n","- InferenceProcessor: test, label 열로 구성된 데이터프레임 넣어서 텍스트와 라벨을 연결해주는 클래스\n","- get_absa_loader: batch size에 따라 dataloader로 변환하는 함수"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":579},"executionInfo":{"elapsed":11963,"status":"ok","timestamp":1703997693937,"user":{"displayName":"Trip Builder","userId":"18334374602196460472"},"user_tz":-540},"id":"CHblAvvw8MFj","outputId":"9dc2c289-37e0-43e1-ba3b-fba01272c2eb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                    text  \\\n","0                     유통기한도 넉넉하고 구성도 많아서 선물 하기 좋네요 만족합니다   \n","1                                    구성도 알차고 촉촉하고 너무 좋아용   \n","2      대용량으로 넉넉하게 사용할 수 있고 무난하고 순한 편이네요 제품 구성은 좋으나 가격...   \n","3             참존을 이 구성에 이 가격으로 사도 되나 싶은 생각이 드는 제품 양도 많아요   \n","4                                 끈적임 없이 잘 흡수되어 매우 만족합니다   \n","...                                                  ...   \n","14995  얼굴에 열이 나면 트러블이 생겨서 지인 소개로 구입했습니다 ㅇㅇㅇ 마스트가 알로에 ...   \n","14996  수분 부족 지성 피부입니다 사용해 봤는데 자극이 없어 좋습니다 팩하고 세안 했는데 ...   \n","14997  성분이 알로에라 촉촉함 하나는 기대했는데 촉촉하지가 않습니다 피부가 번들거리기만 해...   \n","14998  잘 쓰고 있네요 트러블이 요즘 잦아서 쓰는 중인데 확실히 트러블이 가라앉아서 좋네요...   \n","14999  시트지를 알로에로 만들었다고 해서 구매했습니다 생각보다 점성이 너무 있네요 제형이 ...   \n","\n","                                                  labels  \n","0      [B-POS, E-POS, B-POS, I-POS, I-POS, I-POS, E-P...  \n","1                    [B-POS, E-POS, B-POS, I-POS, E-POS]  \n","2      [B-POS, I-POS, I-POS, I-POS, E-POS, B-POS, I-P...  \n","3      [O, B-POS, I-POS, I-POS, I-POS, I-POS, I-POS, ...  \n","4             [B-POS, E-POS, B-POS, I-POS, I-POS, E-POS]  \n","...                                                  ...  \n","14995  [O, O, O, O, O, O, O, O, O, O, B-NEG, I-NEG, I...  \n","14996  [O, O, O, O, O, O, B-POS, I-POS, E-POS, O, O, ...  \n","14997  [B-NEG, I-NEG, I-NEG, I-NEG, I-NEG, B-NEG, E-N...  \n","14998  [O, O, O, O, O, O, O, O, O, B-POS, I-POS, E-PO...  \n","14999  [O, O, O, O, O, O, B-NEG, I-NEG, E-NEG, B-NEG,...  \n","\n","[15000 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-0548e451-90b5-48e7-980b-cb9bd4f689a0\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>유통기한도 넉넉하고 구성도 많아서 선물 하기 좋네요 만족합니다</td>\n","      <td>[B-POS, E-POS, B-POS, I-POS, I-POS, I-POS, E-P...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>구성도 알차고 촉촉하고 너무 좋아용</td>\n","      <td>[B-POS, E-POS, B-POS, I-POS, E-POS]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>대용량으로 넉넉하게 사용할 수 있고 무난하고 순한 편이네요 제품 구성은 좋으나 가격...</td>\n","      <td>[B-POS, I-POS, I-POS, I-POS, E-POS, B-POS, I-P...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>참존을 이 구성에 이 가격으로 사도 되나 싶은 생각이 드는 제품 양도 많아요</td>\n","      <td>[O, B-POS, I-POS, I-POS, I-POS, I-POS, I-POS, ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>끈적임 없이 잘 흡수되어 매우 만족합니다</td>\n","      <td>[B-POS, E-POS, B-POS, I-POS, I-POS, E-POS]</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>14995</th>\n","      <td>얼굴에 열이 나면 트러블이 생겨서 지인 소개로 구입했습니다 ㅇㅇㅇ 마스트가 알로에 ...</td>\n","      <td>[O, O, O, O, O, O, O, O, O, O, B-NEG, I-NEG, I...</td>\n","    </tr>\n","    <tr>\n","      <th>14996</th>\n","      <td>수분 부족 지성 피부입니다 사용해 봤는데 자극이 없어 좋습니다 팩하고 세안 했는데 ...</td>\n","      <td>[O, O, O, O, O, O, B-POS, I-POS, E-POS, O, O, ...</td>\n","    </tr>\n","    <tr>\n","      <th>14997</th>\n","      <td>성분이 알로에라 촉촉함 하나는 기대했는데 촉촉하지가 않습니다 피부가 번들거리기만 해...</td>\n","      <td>[B-NEG, I-NEG, I-NEG, I-NEG, I-NEG, B-NEG, E-N...</td>\n","    </tr>\n","    <tr>\n","      <th>14998</th>\n","      <td>잘 쓰고 있네요 트러블이 요즘 잦아서 쓰는 중인데 확실히 트러블이 가라앉아서 좋네요...</td>\n","      <td>[O, O, O, O, O, O, O, O, O, B-POS, I-POS, E-PO...</td>\n","    </tr>\n","    <tr>\n","      <th>14999</th>\n","      <td>시트지를 알로에로 만들었다고 해서 구매했습니다 생각보다 점성이 너무 있네요 제형이 ...</td>\n","      <td>[O, O, O, O, O, O, B-NEG, I-NEG, E-NEG, B-NEG,...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>15000 rows × 2 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0548e451-90b5-48e7-980b-cb9bd4f689a0')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-0548e451-90b5-48e7-980b-cb9bd4f689a0 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-0548e451-90b5-48e7-980b-cb9bd4f689a0');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-9a87a919-4946-4333-9078-7d86372aacf9\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9a87a919-4946-4333-9078-7d86372aacf9')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-9a87a919-4946-4333-9078-7d86372aacf9 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_bab20f1d-3fad-4220-a0ba-2d1aa6cea4f3\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_total')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_bab20f1d-3fad-4220-a0ba-2d1aa6cea4f3 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df_total');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":4}],"source":["import ast\n","\n","df_total = pd.DataFrame(columns=['text', 'labels'])\n","\n","for i in range(1, 16):\n","  df_test = pd.read_csv('/content/drive/MyDrive/Customizing STTI/test/tag2label_'+str(i)+'.csv')\n","  df_total = pd.concat([df_total, df_test])\n","\n","df_total.reset_index(drop=True, inplace=True)\n","\n","for i in range(len(df_total)):\n","  raw_label = df_total.loc[i, 'labels']\n","  df_total.loc[i, 'labels'] = ast.literal_eval(raw_label)\n","\n","df_total"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":301,"status":"ok","timestamp":1703931905886,"user":{"displayName":"Trip Builder","userId":"18334374602196460472"},"user_tz":-540},"id":"QA9ql6Pz8-z_","outputId":"94635cb1-f165-44ba-f5ab-b0856a3100b0"},"outputs":[{"data":{"text/plain":["0"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["df_total[['text']].duplicated().sum()"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":471,"status":"ok","timestamp":1703982532763,"user":{"displayName":"Trip Builder","userId":"18334374602196460472"},"user_tz":-540},"id":"2peJ9E9_vCGU"},"outputs":[],"source":["for i in range(len(df_total)):\n","  text = df_total.loc[i, 'text']\n","  labels = df_total.loc[i, 'labels']\n","\n","  assert len(text.split(' ')) == len(labels), i"]},{"cell_type":"markdown","metadata":{"id":"cz5OQ6BrWfDj"},"source":["# 임베딩"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":587,"status":"ok","timestamp":1703946976841,"user":{"displayName":"Trip Builder","userId":"18334374602196460472"},"user_tz":-540},"id":"ha2oiy-AJpwE","outputId":"cccafe0e-056d-47da-9705-fd19fa6ae4de"},"outputs":[{"data":{"text/plain":["{'O': 0,\n"," 'EQ': 1,\n"," 'B-POS': 2,\n"," 'I-POS': 3,\n"," 'E-POS': 4,\n"," 'S-POS': 5,\n"," 'B-NEG': 6,\n"," 'I-NEG': 7,\n"," 'E-NEG': 8,\n"," 'S-NEG': 9,\n"," 'B-NEU': 10,\n"," 'I-NEU': 11,\n"," 'E-NEU': 12,\n"," 'S-NEU': 13}"]},"execution_count":77,"metadata":{},"output_type":"execute_result"}],"source":["label_list = processor.get_labels()\n","label_map = {label:i for i, label in enumerate(label_list)}\n","label_map"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1703946993552,"user":{"displayName":"Trip Builder","userId":"18334374602196460472"},"user_tz":-540},"id":"NgDX5pQ0J54l","outputId":"00a70c0f-3b6d-4c3c-bf33-630780ca2a51"},"outputs":[{"data":{"text/plain":["0"]},"execution_count":78,"metadata":{},"output_type":"execute_result"}],"source":["label_map['O']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1703947149730,"user":{"displayName":"Trip Builder","userId":"18334374602196460472"},"user_tz":-540},"id":"rVA8KU5AKFuT","outputId":"fda91dbc-fc1e-4434-8641-671a2e2860bb"},"outputs":[{"data":{"text/plain":["[2, 3, 4, 2, 3, 3, 4, 10, 11, 11, 11, 11, 11, 11, 12]"]},"execution_count":79,"metadata":{},"output_type":"execute_result"}],"source":["a = ['B-POS', 'I-POS', 'E-POS', 'B-POS', 'I-POS', 'I-POS', 'E-POS', 'B-NEU', 'I-NEU', 'I-NEU', 'I-NEU', 'I-NEU', 'I-NEU', 'I-NEU', 'E-NEU']\n","label_ids = [label_map[label] for label in a]\n","label_ids"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1703947161805,"user":{"displayName":"Trip Builder","userId":"18334374602196460472"},"user_tz":-540},"id":"so6j7UQEKjPF","outputId":"84630781-9971-4927-abb1-4809c9ad2318"},"outputs":[{"data":{"text/plain":["15"]},"execution_count":80,"metadata":{},"output_type":"execute_result"}],"source":["len(label_ids)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1703997693937,"user":{"displayName":"Trip Builder","userId":"18334374602196460472"},"user_tz":-540},"id":"jDZl7CfenK8w"},"outputs":[],"source":["def clean_text(raw_text):\n","  text = re.sub(r'[.]', ' ', raw_text)\n","  text = re.sub(r'[^ㄱ-ㅣ가-힣\\s]', ' ', text)\n","  text = re.sub(' +', ' ', text)\n","  return text\n","\n","class InputExample:\n","    def __init__(self, text_a, label=None):\n","        '''\n","        Inputs:\n","            guid: Unique id for the example.\n","            text_a: string. The untokenized text of the first sequence. For single\n","                    sequence tasks, only this sequence must be specified.\n","            text_b: (Optional) string. The untokenized text of the second sequence.\n","            label: (Optional) string. The label of the example. This should be specified for train and dev examples.\n","        '''\n","        self.text_a = text_a\n","        self.label = label"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1703997693937,"user":{"displayName":"Trip Builder","userId":"18334374602196460472"},"user_tz":-540},"id":"WI41iMY_K7-d"},"outputs":[],"source":["class SeqInputFeatures:\n","    def __init__(self, input_ids, input_mask, segment_ids, label_ids, evaluate_label_ids):\n","        self.input_ids = input_ids\n","        self.input_mask = input_mask\n","        self.segment_ids = segment_ids\n","        self.label_ids = label_ids\n","        self.evaluate_label_ids = evaluate_label_ids\n","\n","def convert_examples_to_seq_features(examples, label_list, tokenizer):\n","  label_map = {label:i for i, label in enumerate(label_list)}\n","  features = []\n","  max_seq_length = -1\n","  texts_tokenized = []\n","\n","  cls_token='[CLS]'\n","  sep_token='[SEP]'\n","  cls_token_segment_id = 1\n","  sequence_a_segment_id = 0\n","  pad_token_segment_id = 0\n","  pad_token = 0\n","\n","  for example in examples:\n","    tokens_total, labels_total = [], []\n","    evaluate_label_ids = []\n","\n","    text = example.text_a\n","    words = example.text_a.split(' ')\n","    labels = example.label\n","\n","    wid, tid = 0, 0\n","    for word, label in zip(words, example.label):   # 단어랑 태그를 하나씩 묶음\n","      subwords = tokenizer.tokenize(word)           # 단어 토크나이징 -> 서브단어 생길 수 있음\n","      tokens_total.extend(subwords)                 # 최종 토큰에 붙여 넣음\n","      if label != 'O':                              # 라벨이 O이 아니면\n","        labels_total.extend([label] + (['EQ'] * (len(subwords) - 1)))   # 서브워드 생긴만큼 EQ 태그를 뒤에 삽입\n","      else:                                         # 라벨이 O 이면\n","        labels_total.extend(['O'] * len(subwords))  # 서브워드 길이만큼 O 삽입\n","      evaluate_label_ids.append(tid)                # evaluate_label_ids에 tid 삽입\n","      wid += 1                                      # wid 1 증가\n","      tid += len(subwords)                          # 서브워드 길이만큼 tid 증가\n","    assert tid == len(tokens_total)                 # 최종 tid가 토큰수와 같은지\n","\n","    evaluate_label_ids = np.array(evaluate_label_ids, dtype=np.int32) # 어레이로 바꿈? (어레이: 연속적으로 저장)\n","    texts_tokenized.append((tokens_total, labels_total, evaluate_label_ids)) # 종합해서 저장\n","\n","    if len(tokens_total) > max_seq_length:          # 토큰 길이가 최대 문장 길이보다 길면\n","      max_seq_length = len(tokens_total)            # 최대 문장 길이 갱신\n","\n","  assert len(examples) == len(texts_tokenized)\n","  max_seq_length += 2                               # cls, sep 토큰 2개 추가\n","  for idx, (tokens_a, labels_a, evaluate_label_ids) in enumerate(texts_tokenized):\n","    tokens = ['[CLS]'] + tokens_a + ['[SEP]']\n","    labels = ['O'] + labels_a  + ['O']\n","    segment_ids = [cls_token_segment_id] + [sequence_a_segment_id] * (len(tokens)-1)\n","    evaluate_label_ids += 1\n","\n","    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","    input_mask = [1] * len(input_ids)\n","    padding_length = max_seq_length - len(input_ids)\n","    label_ids = [label_map[label] for label in labels]\n","\n","    input_ids = input_ids + ([pad_token] * padding_length)\n","    input_mask = input_mask + ([0] * padding_length)\n","    segment_ids = segment_ids + ([pad_token_segment_id] * padding_length)\n","\n","    label_ids = label_ids + ([0] * padding_length)\n","\n","    assert len(input_ids) == max_seq_length,    text\n","    assert len(input_mask) == max_seq_length,   text\n","    assert len(segment_ids) == max_seq_length,  text\n","    assert len(label_ids) == max_seq_length,    labels_a\n","\n","    features.append(SeqInputFeatures(input_ids=input_ids,\n","                                     input_mask=input_mask,\n","                                     segment_ids=segment_ids,\n","                                     label_ids=label_ids,\n","                                     evaluate_label_ids=evaluate_label_ids))\n","  #print('maximal sequence length is %d' % (max_seq_length))\n","\n","  return features"]},{"cell_type":"code","source":["processor = DataProcessor()\n","label_list = processor.get_labels()\n","num_labels = len(label_list)\n","\n","tokenizer = BertTokenizer.from_pretrained('klue/bert-base')\n","\n","error_list = []\n","\n","for i in range(len(df_total)):\n","  try:\n","    examples_t = processor.get_examples(df_total.loc[i:i, :])\n","    features_t = convert_examples_to_seq_features(examples=examples_t, label_list=label_list, tokenizer=tokenizer)\n","  except AssertionError as e:\n","    error_list.append(i)"],"metadata":{"id":"HztUr9p-WUy6","executionInfo":{"status":"ok","timestamp":1703997794473,"user_tz":-540,"elapsed":47817,"user":{"displayName":"Trip Builder","userId":"18334374602196460472"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["error_list"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AWCMmZKqeBpI","executionInfo":{"status":"ok","timestamp":1703997794473,"user_tz":-540,"elapsed":11,"user":{"displayName":"Trip Builder","userId":"18334374602196460472"}},"outputId":"b2cf91db-7712-4aae-c1fd-91ab234862b5"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[599,\n"," 1846,\n"," 2968,\n"," 3845,\n"," 4057,\n"," 4296,\n"," 4414,\n"," 5203,\n"," 5443,\n"," 7322,\n"," 11962,\n"," 12539,\n"," 12565,\n"," 13137,\n"," 14680]"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["len(error_list)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mDeycJqMeaZA","executionInfo":{"status":"ok","timestamp":1703997794473,"user_tz":-540,"elapsed":10,"user":{"displayName":"Trip Builder","userId":"18334374602196460472"}},"outputId":"ca9e2dff-4a10-49d0-ae3b-81108a1f7d1a"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["15"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["df_total.drop(error_list, inplace=True)"],"metadata":{"id":"O96Ne4MNYugi","executionInfo":{"status":"ok","timestamp":1703997806792,"user_tz":-540,"elapsed":376,"user":{"displayName":"Trip Builder","userId":"18334374602196460472"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["len(df_total)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YR8JBnh2YUUq","executionInfo":{"status":"ok","timestamp":1703997812011,"user_tz":-540,"elapsed":2,"user":{"displayName":"Trip Builder","userId":"18334374602196460472"}},"outputId":"21642c70-5f06-4518-87d2-d83f21472f21"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["14985"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["torch.save(features_t, '/content/test_data.pt')"],"metadata":{"id":"GPjsbRoCYOcS","executionInfo":{"status":"ok","timestamp":1703984315420,"user_tz":-540,"elapsed":1039,"user":{"displayName":"Trip Builder","userId":"18334374602196460472"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1703997705314,"user":{"displayName":"Trip Builder","userId":"18334374602196460472"},"user_tz":-540},"id":"m8GouCkRmmF1"},"outputs":[],"source":["'''\n","tensor([[    2,  4743,  2015,  ...,     0,     0,     0],\n","        [    2,  3896,  2119,  ...,     0,     0,     0],\n","        [    2, 18059,  6233,  ...,     0,     0,     0],\n","        ...,\n","        [    2,  3977,  2170,  ...,     0,     0,     0],\n","        [    2,  5411, 31221,  ...,     0,     0,     0],\n","        [    2,  1443,  4514,  ...,     0,     0,     0]])\n","tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]])\n","tensor([[1, 0, 0,  ..., 0, 0, 0],\n","        [1, 0, 0,  ..., 0, 0, 0],\n","        [1, 0, 0,  ..., 0, 0, 0],\n","        ...,\n","        [1, 0, 0,  ..., 0, 0, 0],\n","        [1, 0, 0,  ..., 0, 0, 0],\n","        [1, 0, 0,  ..., 0, 0, 0]])'''\n","\n","class DataProcessor:\n","    def get_examples(self, dataframe):\n","        return self._create_examples(dataframe)\n","\n","    def get_labels(self):\n","        return ['O', 'EQ', 'B-POS', 'I-POS', 'E-POS', 'S-POS',\n","                 'B-NEG', 'I-NEG', 'E-NEG', 'S-NEG',\n","                 'B-NEU', 'I-NEU', 'E-NEU', 'S-NEU']\n","\n","    def _create_examples(self, dataframe):\n","        examples = []\n","        dataframe.reset_index(drop=True, inplace=True)\n","\n","        for sample_id, sentence in enumerate(dataframe.loc[:, 'text'].values):\n","            label = dataframe.loc[sample_id, 'labels']\n","            examples.append(InputExample(text_a = sentence, label=label))\n","\n","        return examples\n","\n","def get_absa_loader(dataframe, tokenizer, mode, batch_size):\n","  processor = DataProcessor()\n","  examples = processor.get_examples(dataframe)\n","  label_list = processor.get_labels()\n","  features = convert_examples_to_seq_features(examples=examples, label_list=label_list, tokenizer=tokenizer)\n","  torch.save(features, '/content/'+str(mode)+'.pt')\n","\n","  all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n","  all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n","  all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n","\n","  all_label_ids = torch.tensor([f.label_ids for f in features], dtype=torch.long)\n","  all_evaluate_label_ids = [f.evaluate_label_ids for f in features]\n","  dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n","\n","  if mode=='train':\n","    sampler = RandomSampler(dataset)\n","  elif mode=='valid' or mode=='test':\n","    sampler = SequentialSampler(dataset)\n","\n","  dataloader = DataLoader(dataset, sampler=sampler, batch_size=batch_size)\n","\n","  return (dataloader, all_evaluate_label_ids)"]},{"cell_type":"code","source":[],"metadata":{"id":"lOXuL4RgWBZg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bYRGXysSy7O8"},"source":["# BERT layer"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1703997708534,"user":{"displayName":"Trip Builder","userId":"18334374602196460472"},"user_tz":-540},"id":"s-xJEfJa22ao"},"outputs":[],"source":["from transformers import PreTrainedModel, BertModel, BertConfig\n","from torch.utils.data import Dataset\n","\n","import torch\n","from torch import nn\n","from transformers import BertModel"]},{"cell_type":"markdown","metadata":{"id":"iGbOOjgnaj5l"},"source":["- ABSAProcessor: 논문 태그로 바꾸고 example 생성\n","- InferenceProcessor: 이미 논문 태그로 바뀌어 있는 dataframe 사용해서 example 생성\n","\n","- 모델을 학습시키기 위한 요소들을 명시한 json 파일로 되어 있음\n","- batch size, learning rate, weigth_decay 등 train에 필요한 요소들부터 tokenizer에 특수 토큰 (special token eg[mask] ) 들을 미리 설정하는 등 설정에 관한 전반적인 것들 명시\n","- PretrainedModel을 save_pretrained 메소드를 이용하면 모델의 체크포인트와 함께 저장되도록 되어있음.\n","- hugging face의 pretrained model을 그대로 사용하게 되면 자동으로 config파일이 로드되어 명시할 필요가 없지만, 설정을 변경하고 싶거나 나만의 모델을 학습시킬 때에는 config파일을 직접 불러와야 함.\n","- config 또한 model, tokenizer처럼 Model ID만 있으면, Config 클래스를 명확히 지정하거나 혹은 AutoConfig를 이용하는 방식으로 불러올 수 있음\n","\n","- 기본적으로 모델들은 PretrainedModel 클래스를 상속받고 있음\n","- PretrainedModel 클래스는 학습된 모델을 불러오고, 다운로드하고, 저장하는 등 모델 전반에 걸쳐 적용되는 메소드를 가지고 있음\n","- 상속 구조를 가지고 있기 때문에, 실제로 사용할 모델이 BERT 이건 GPT 이건 상관없이 모델을 불러오고 다운로드/저장하는 등의 작업에 활용하는 메소드는 부모 클래스의 것을 동일하게 활용\n","\n","https://velog.io/@olxtar/PyTorch-class%EB%A1%9C-%EC%83%9D%EC%84%B1%EB%90%9C-model%EC%9D%98-layer-weights-%EC%A0%91%EA%B7%BC%EB%B2%95\n","\n","https://tutorials.pytorch.kr/beginner/former_torchies/tensor_tutorial_old.html"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1703997709100,"user":{"displayName":"Trip Builder","userId":"18334374602196460472"},"user_tz":-540},"id":"yBK0acd02sS1"},"outputs":[],"source":["class TaggerConfig:\n","    def __init__(self):\n","        self.hidden_dropout_prob = 0.1\n","        self.hidden_size = 768\n","\n","class BertLayerNorm(nn.Module): # hiddem layer 전체의 평균과 분산으로 normalization함\n","    def __init__(self, hidden_size, eps=1e-12):\n","        super(BertLayerNorm, self).__init__()\n","        self.weight = nn.Parameter(torch.ones(hidden_size)) # torch.nn.parameter.Parameter\n","        self.bias = nn.Parameter(torch.zeros(hidden_size))\n","        self.variance_epsilon = eps # 분모가 0이 되는 경우를 방지하기 위한 아주 작은 상수\n","\n","    def forward(self, x): # input을 넣고 어떤 과정을 거쳐 output이 나올지를 정의해준다는 느낌\n","        u = x.mean(-1, keepdim=True) # 입력값 x(텐서)의 마지막 차원 (-1)을 따라 평균을 구하고 차원을 유지하는 함수. keepdim=True인 경우 출력 텐서는 크기가 1인 dim 차원을 제외하고 input과 동일한 크기\n","        s = (x - u).pow(2).mean(-1, keepdim=True) # 왜 -1 쓰는지? -> 열벡터로 출력하기 위해\n","        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n","        return self.weight * x + self.bias # 일종의 예측값(y_hat)으로 볼 수 있다.\n","\n","class BertPreTrainedModel(PreTrainedModel):\n","    config_class = BertConfig\n","    base_model_prefix = \"bert\"\n","\n","    def __init__(self, *inputs, **kwargs):\n","        super(BertPreTrainedModel, self).__init__(*inputs, **kwargs)\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, (nn.Linear, nn.Embedding)):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range) # 가중치 초기화\n","        elif isinstance(module, BertLayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","        if isinstance(module, nn.Linear) and module.bias is not None:\n","            module.bias.data.zero_()\n","\n","class BertABSATagger(BertPreTrainedModel):\n","    def __init__(self, bert_config): # config 불러옴\n","        super(BertABSATagger, self).__init__(bert_config)\n","        self.num_labels = bert_config.num_labels\n","        self.tagger_config = TaggerConfig()\n","        self.bert = BertModel(bert_config) # 해당 config로 모델을 생성\n","        self.bert_dropout = nn.Dropout(bert_config.hidden_dropout_prob) # 드랍아웃 설정\n","        if bert_config.fix_tfm:\n","            for p in self.bert.parameters():\n","                p.required_grad = False  # Frizen 미분 안되도록?\n","\n","        self.tagger_dropout = nn.Dropout(self.tagger_config.hidden_dropout_prob) # tagger 드랍 아웃이 왜 따로 있지?\n","        self.tagger = SAN(model=bert_config.hidden_size, nhead=12, dropout=0.1)\n","        penultimate_hidden_size = self.tagger_config.hidden_size\n","        self.classifier = nn.Linear(penultimate_hidden_size, self.num_labels) # 마지막 레이어\n","\n","    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None, position_ids=None, head_mask=None):\n","        outputs = self.bert(\n","            input_ids,\n","            position_ids=position_ids,\n","            token_type_ids=token_type_ids,\n","            attention_mask=attention_mask,\n","            head_mask=head_mask\n","        ) # 정의한 bert 모델에 입력값 넣음 -> 넣으면 뭐가 되는데?\n","        # Transformer의 인코더를 이용하여 구축된 BERT를 사용\n","\n","        # 기존 연구에서 제안된 E2E-ABSA 모델과 같이 BERT를 사용하고 마지막에 single sequence tagger를 통과해 속성을 예측\n","        tagger_input = outputs[0] # pooler\n","        tagger_input = self.bert_dropout(tagger_input)\n","        tagger_input = tagger_input.transpose(0, 1) # 각 성분에 대해서 classification을 하기 위함.\n","        classifier_input = self.tagger(tagger_input)\n","        classifier_input = classifier_input.transpose(0, 1)\n","        classifier_input = self.tagger_dropout(classifier_input)\n","        logits = self.classifier(classifier_input) # 소프트맥스 통과는 어디?\n","\n","        outputs = (logits, ) + outputs[2:]\n","\n","        if labels is not None:\n","            criterion = nn.CrossEntropyLoss()\n","\n","            if attention_mask is not None:\n","                active_loss = attention_mask.view(-1) == 1\n","                active_logits = logits.view(-1, self.num_labels)[active_loss]\n","                active_labels = labels.view(-1)[active_loss]\n","                loss = criterion(active_logits, active_labels)\n","\n","            else:\n","                loss = criterion(logits.view(-1, self.num_labels), labels.view(-1))\n","\n","            outputs = (loss, ) + outputs\n","\n","        return outputs\n","\n","class SAN(nn.Module):\n","    def __init__(self, model, nhead, dropout=0.1):\n","        super(SAN, self).__init__()\n","        self.model = model\n","        self.self_attn = nn.MultiheadAttention(model, nhead, dropout=dropout)\n","        self.dropout = nn.Dropout(p=dropout)\n","        self.norm = nn.LayerNorm(model)\n","\n","    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n","        src2, _ = self.self_attn(src, src, src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask) # (key, query, value)\n","        src = src + self.dropout(src2)\n","        src = self.norm(src)\n","        return src"]},{"cell_type":"markdown","metadata":{"id":"pUmWWKf1zya7"},"source":["# train"]},{"cell_type":"markdown","metadata":{"id":"OOA0-nx9nvY0"},"source":["os 모듈은 Operating System의 약자로서 운영체제에서 제공되는 여러 기능을 파이썬에서 수행할 수 있게 해줍니다.\n","\n","예를 들어, 파이썬을 이용해 파일을 복사하거나 디렉터리를 생성하고 특정 디렉터리 내의 파일 목록을 구하고자 할 때 os 모듈을 사용하면 됩니다."]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1703997709458,"user":{"displayName":"Trip Builder","userId":"18334374602196460472"},"user_tz":-540},"id":"18K8vOuGmbH6"},"outputs":[],"source":["import os\n","import random\n","import easydict\n","from tqdm import tqdm\n","from torch.nn.utils import clip_grad_norm_\n","from transformers import get_linear_schedule_with_warmup"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1703997709883,"user":{"displayName":"Trip Builder","userId":"18334374602196460472"},"user_tz":-540},"id":"O9B9hBgVqp8P"},"outputs":[],"source":["SMALL_POSITIVE_CONST = 1e-4\n","\n","args = easydict.EasyDict({\"absa_type\": 'san',\n","                          \"model_type\": 'bert',\n","                          \"fix_tfm\": 0,\n","                          \"max_seq_length\": 512,\n","                          \"num_epochs\": 100,\n","                          \"batch_size\": 128,\n","                          \"save_steps\": 100,\n","                          \"seed\": 42,\n","                          \"epsilon\": 1e-8,\n","                          \"warmup_steps\": 0,\n","                          \"model_name_or_path\": 'klue/bert-base',\n","                          \"max_grad_norm\": 1.0,\n","                          \"learning_rate\": 1e-5,\n","                          \"device\": 'cuda'})\n","\n","BASE_DIR = os.getcwd()\n","#SEM_DIR = os.path.join(BASE_DIR, 'SemEval')\n","#YELP_DIR = os.path.join(BASE_DIR, 'Yelp')\n","PARAM_DIR = os.path.join(BASE_DIR, 'bert-san')\n","RESULT_DIR = os.path.join(BASE_DIR, 'results')"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1703997709883,"user":{"displayName":"Trip Builder","userId":"18334374602196460472"},"user_tz":-540},"id":"70nHiBYzm83Y"},"outputs":[],"source":["def match_ts(gold_ts_sequence, pred_ts_sequence):\n","    '''\n","    Inputs:\n","        gold_ts_sequence: gold standard targeted sentiment sequence (ground truth)\n","        pred_ts_sequence: predicted targeted sentiment sequence\n","    '''\n","\n","    tag2tagid = {'POS': 0, 'NEG': 1, 'NEU': 2}\n","    hit_count, gold_count, pred_count = np.zeros(3), np.zeros(3), np.zeros(3)\n","    for t in gold_ts_sequence:\n","        ts_tag = t[2]\n","        tid = tag2tagid[ts_tag]\n","        gold_count[tid] += 1\n","\n","    for t in pred_ts_sequence:\n","        ts_tag = t[2]\n","        tid = tag2tagid[ts_tag]\n","        if t in gold_ts_sequence:\n","            hit_count[tid] += 1\n","        pred_count[tid] += 1\n","    return hit_count, gold_count, pred_count\n","\n","def compute_metrics_absa(preds, labels, all_evaluate_label_ids):\n","    absa_label_vocab = {'O':0, 'EQ':1, 'B-POS':2, 'I-POS':3, 'E-POS':4, 'S-POS':5,\n","                        'B-NEG':6, 'I-NEG':7, 'E-NEG':8, 'S-NEG':9, 'B-NEU':10,\n","                        'I-NEU':11, 'E-NEU':12, 'S-NEU':13}\n","    absa_id2tag = {}\n","    for k in absa_label_vocab:\n","        v = absa_label_vocab[k]\n","        absa_id2tag[v] = k\n","\n","    n_tp_ts, n_gold_ts, n_pred_ts = np.zeros(3), np.zeros(3), np.zeros(3)\n","    ts_precision, ts_recall, ts_f1 = np.zeros(3), np.zeros(3), np.zeros(3)\n","    n_samples = len(all_evaluate_label_ids)\n","\n","    class_count = np.zeros(3)\n","\n","    for i in range(n_samples):\n","        evaluate_label_ids = all_evaluate_label_ids[i]\n","        pred_labels = preds[i][evaluate_label_ids]\n","        gold_labels = labels[i][evaluate_label_ids]\n","        assert len(pred_labels) == len(gold_labels)\n","\n","        pred_tags = [absa_id2tag[label] for label in pred_labels]\n","        gold_tags = [absa_id2tag[label] for label in gold_labels]\n","\n","        g_ts_sequence, p_ts_sequence = tag2ts(ts_tag_sequence=gold_tags), tag2ts(ts_tag_sequence=pred_tags)\n","\n","        hit_ts_count, gold_ts_count, pred_ts_count = match_ts(gold_ts_sequence=g_ts_sequence,\n","                                                              pred_ts_sequence=p_ts_sequence)\n","\n","        n_tp_ts += hit_ts_count\n","        n_gold_ts += gold_ts_count\n","        n_pred_ts += pred_ts_count\n","\n","        for (_, _, s) in g_ts_sequence:\n","            if s == 'POS':\n","                class_count[0] += 1\n","            if s == 'NEG':\n","                class_count[1] += 1\n","            if s == 'NEU':\n","                class_count[2] += 1\n","\n","    for i in range(3):\n","        n_ts = n_tp_ts[i]\n","        n_g_ts = n_gold_ts[i]\n","        n_p_ts = n_pred_ts[i]\n","        ts_precision[i] = float(n_ts) / float(n_p_ts +SMALL_POSITIVE_CONST)\n","        ts_recall[i] = float(n_ts) / float(n_g_ts + SMALL_POSITIVE_CONST)\n","        ts_f1[i] = 2 * ts_precision[i] * ts_recall[i] / (ts_precision[i] + ts_recall[i] + SMALL_POSITIVE_CONST)\n","\n","    macro_f1 = ts_f1.mean()\n","\n","    n_tp_total = sum(n_tp_ts)\n","    n_g_total = sum(n_gold_ts)\n","    print('class_count:', class_count)\n","\n","    n_p_total = sum(n_pred_ts)\n","    micro_p = float(n_tp_total) / (n_p_total + SMALL_POSITIVE_CONST)\n","    micro_r = float(n_tp_total) / (n_g_total + SMALL_POSITIVE_CONST)\n","    micro_f1 = 2 * micro_p * micro_r / (micro_p + micro_r + SMALL_POSITIVE_CONST)\n","    scores = {'macro-f1':macro_f1, 'precision':micro_p, 'recall':micro_r, 'micro-f1':micro_f1}\n","\n","    return scores\n","\n","def set_seed(args):\n","  random.seed(args.seed)\n","  np.random.seed(args.seed)\n","  torch.manual_seed(args.seed)\n","  torch.cuda.manual_seed_all(args.seed)"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1703997710469,"user":{"displayName":"Trip Builder","userId":"18334374602196460472"},"user_tz":-540},"id":"0D4zCo41gSJI"},"outputs":[],"source":["def tag2ts(ts_tag_sequence): # ts_tag_sequence: [O, O, S-NEG, O, O, O, O, O, O]\n","    n_tags = len(ts_tag_sequence) # 9\n","    ts_sequence, sentiments = [], []\n","    beg, end = -1, -1\n","\n","    for i in range(n_tags):\n","        ts_tag = ts_tag_sequence[i]\n","        eles = ts_tag.split('-') # ['O'] 혹은 ['S', 'NEG']\n","\n","        if len(eles) == 2: # 태그가 O이 아니면\n","            pos, sentiment = eles\n","            # pos: S\n","            # sentiment: NEG\n","        else: # 태그가 O이면\n","            pos, sentiment = 'O', 'O'\n","\n","        if sentiment != 'O': # 태그가 O이 아니면, sentiment: NEG\n","            sentiments.append(sentiment) # sentiment = ['NEG']\n","\n","        if pos == 'S': # Single tag이면\n","            ts_sequence.append((i, i, sentiment)) # 튜플 추가, [(2, 2, 'NEG')]\n","            sentiments = [] # 초기화\n","\n","        elif pos == 'B': # Beginning tag이면\n","            beg = i\n","            if len(sentiments) > 1: # 앞의 태그가 O이 여러개였다면 ['O', ..., 'O'] 이렇게 붙었을 것\n","                sentiments = [sentiments[-1]] # 마지막 한 개만 가져옴\n","\n","        elif pos == 'E': # Ending tag이면\n","            end = i\n","\n","            if end > beg > -1 and len(set(sentiments)) == 1:\n","                ts_sequence.append((beg, end, sentiment)) # 위치표시, [(3, 5, 'NEG')]\n","                sentiments = []\n","                beg, end = -1, -1\n","\n","    return ts_sequence # [(2, 2, 'NEG'), (3, 5, 'NEG')]"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1703997711106,"user":{"displayName":"Trip Builder","userId":"18334374602196460472"},"user_tz":-540},"id":"cr2c4XnZzx6j"},"outputs":[],"source":["# TaggerConfig\n","# BertLayerNorm\n","# BertPreTrainedModel\n","# BertABSATagger\n","\n","def absa_train(args, model, train_loader, eval_loader, optimizer):\n","\n","    t_total = len(train_loader) * args.num_epochs\n","\n","    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)\n","    best_loss = float(np.inf)\n","    global_step = 0\n","    train_loss_list, eval_loss_list = [], []\n","    results_list = []\n","    set_seed(args)\n","    print('***** Running Training *****')\n","    for epoch in range(1, args.num_epochs + 1):\n","        train_loss, eval_loss = 0.0, 0.0\n","        train_iterator = tqdm(train_loader, desc='training...', disable=False)\n","        model.train()\n","\n","        for batch in train_iterator:\n","\n","            batch = tuple(t.to(args.device) for t in batch)\n","            inputs = {'input_ids':          batch[0],\n","                      'attention_mask':     batch[1],\n","                      'token_type_ids':     batch[2],\n","                      'labels':             batch[3]}\n","\n","            optimizer.zero_grad()\n","            outputs = model(**inputs)\n","            loss = outputs[0]\n","            loss.backward()\n","            clip_grad_norm_(model.parameters(), 1.0)\n","\n","            train_loss += loss.item() / len(batch)\n","            optimizer.step()\n","            scheduler.step()\n","            global_step += 1\n","\n","\n","            # model check-point\n","            if global_step % args.save_steps == 0:\n","                output_dir = os.path.join(args.output_dir, f'checkpoint-{global_step}')\n","                if not os.path.exists(output_dir):\n","                    os.makedirs(output_dir)\n","\n","                model_to_save = model.module if hasattr(model, 'module') else model\n","                model_to_save.save_pretrained(output_dir)\n","                torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n","\n","\n","            train_loss /= len(train_iterator)\n","            train_loss_list.append(train_loss)\n","\n","\n","\n","        model.eval()\n","        eval_iterator = tqdm(eval_loader[0], desc='training...')\n","        preds, out_label_ids = None, None\n","        results = {}\n","        for batch in eval_iterator:\n","            batch = tuple(t.to(args.device) for t in batch)\n","\n","            with torch.no_grad():\n","                inputs = {'input_ids':          batch[0],\n","                        'attention_mask':     batch[1],\n","                        'token_type_ids':     batch[2],\n","                        'labels':             batch[3]}\n","\n","                outputs = model(**inputs)\n","                tmp_eval_loss, logits = outputs[:2]\n","                eval_loss += tmp_eval_loss.mean().item()\n","\n","            if preds is None:\n","                preds = logits.detach().cpu().numpy()\n","                out_label_ids = inputs['labels'].detach().cpu().numpy()\n","\n","            else:\n","                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n","                out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n","\n","        eval_loss /= len(eval_iterator)\n","\n","        eval_loss_list.append(eval_loss)\n","        preds = np.argmax(preds, axis=-1)\n","        result = compute_metrics_absa(preds, out_label_ids, eval_loader[1])\n","        results.update(result)\n","        results_list.append(results)\n","\n","        output_file = os.path.join(args.output_dir, '%s_results.txt' % 'valid')\n","        if not os.path.exists(args.output_dir):\n","            os.makedirs(args.output_dir)\n","\n","        with open(output_file, 'w') as writer:\n","            for key in sorted(result.keys()):\n","                if 'eval_loss' in key:\n","                    print(f'{key} = {str(result[key])}')\n","                writer.write('%s = %s\\n' % (key, str(result[key])))\n","\n","        if best_loss > eval_loss :\n","            torch.save(model.state_dict(), os.path.join(args.output_dir, f'best-parameters.pt'))\n","            best_loss = eval_loss\n","            best_epoch = epoch\n","\n","        print(f'epochs: [{epoch}/{args.num_epochs}], best epoch is {best_epoch}')\n","        print(f'train loss: {train_loss:.5f},\\t valid loss: {eval_loss:.5f}')\n","        print(f'macro-f1: {result[\"macro-f1\"]:.5f},\\tmicro-f1: {result[\"micro-f1\"]:.5f}')\n","        print(f\"recall: {result['recall']:.5f},\\tprecision: {result['precision']:.5f}\")\n","    print('***** Finished Training *****')\n","\n","    return {\n","        'results': results_list,\n","        'train_loss': train_loss_list,\n","        'valid_loss': eval_loss_list\n","    }"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1703997711106,"user":{"displayName":"Trip Builder","userId":"18334374602196460472"},"user_tz":-540},"id":"29-TAgfdtT6J"},"outputs":[],"source":["import pickle\n","from torch import optim"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":525,"status":"ok","timestamp":1703997827796,"user":{"displayName":"Trip Builder","userId":"18334374602196460472"},"user_tz":-540},"id":"F5IbrXie-9Qr"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","df_train, df_test = train_test_split(df_total, test_size=0.2, random_state=11)\n","\n","df_train.reset_index(inplace=True, drop=True)\n","df_test.reset_index(inplace=True, drop=True)"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":588,"referenced_widgets":["2b9cf905f6c442c8b67972d535060edb","b7daeae51a034a828def5236ed379dde","4237d488d4cd4b7a988b57f5523b448f","cefdb53b2d4948eebdcedbb000e5cedd","776edeba834545749420e93a96ab5c72","783e10b81de8409a8134449330971fab","51a864486f42485990789e71683d5ebe","87aad134ae164f509d63dcefa07f9ecf","22a6d4c422d3416ab52b5e066a3acace","948bc41d418243f19572ebf63b3f8c82","cb7410962b9c49aea3f84d56523f6861"]},"executionInfo":{"elapsed":31935,"status":"error","timestamp":1703997859729,"user":{"displayName":"Trip Builder","userId":"18334374602196460472"},"user_tz":-540},"id":"Bucqtcb0tOIf","outputId":"b8e41715-aab9-4dc8-af1b-d9320316a647"},"outputs":[{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/445M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b9cf905f6c442c8b67972d535060edb"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of BertABSATagger were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['tagger.self_attn.out_proj.bias', 'tagger.self_attn.out_proj.weight', 'classifier.weight', 'tagger.norm.bias', 'classifier.bias', 'tagger.norm.weight', 'tagger.self_attn.in_proj_bias', 'tagger.self_attn.in_proj_weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["***** Running Training *****\n"]},{"output_type":"stream","name":"stderr","text":["training...:   0%|          | 0/188 [00:01<?, ?it/s]\n"]},{"output_type":"error","ename":"OutOfMemoryError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-644ff10cbdea>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabsa_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0msave_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRESULT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Customizing STTI/test/valid_results.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-bf62b69653d8>\u001b[0m in \u001b[0;36mabsa_train\u001b[0;34m(args, model, train_loader, eval_loader, optimizer)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-447f6b2023c0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, labels, position_ids, head_mask)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mtagger_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagger_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mtagger_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 각 성분에 대해서 classification을 하기 위함.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mclassifier_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagger_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mclassifier_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mclassifier_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagger_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-447f6b2023c0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0msrc2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (key, query, value)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                 is_causal=is_causal)\n\u001b[1;32m   1240\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1241\u001b[0;31m             attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[0m\u001b[1;32m   1242\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5406\u001b[0m         \u001b[0mattn_output_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdropout_p\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5408\u001b[0;31m             \u001b[0mattn_output_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5410\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1264\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"dropout probability has to be between 0 and 1, but got {p}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1266\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 168.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 101.06 MiB is free. Process 7591 has 14.65 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 409.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["output_dir = f'{args.model_type}-{args.absa_type}'\n","args.output_dir = output_dir\n","\n","processor = DataProcessor()\n","label_list = processor.get_labels()\n","num_labels = len(label_list)\n","\n","config_class, model_class, tokenizer_class = BertConfig, BertABSATagger, BertTokenizer\n","config = config_class.from_pretrained(args.model_name_or_path, num_labels=num_labels)\n","tokenizer = tokenizer_class.from_pretrained(args.model_name_or_path)\n","\n","config.absa_type = args.absa_type\n","config.fix_tfm = args.fix_tfm\n","\n","model = model_class.from_pretrained(args.model_name_or_path, config=config)\n","\n","model = model.to(args.device)\n","batch_size = 64\n","train_dataloader, _ = get_absa_loader(df_train, tokenizer, mode='train', batch_size=batch_size)\n","valid_dataloader = get_absa_loader(df_test, tokenizer, mode='valid', batch_size=batch_size)\n","optimizer = optim.AdamW(model.parameters(), lr=args.learning_rate)\n","\n","results = absa_train(args, model, train_dataloader, valid_dataloader, optimizer)\n","\n","save_dir = os.path.join(RESULT_DIR, '/content/drive/MyDrive/Customizing STTI/test/valid_results.pkl')\n","if not os.path.exists(RESULT_DIR):\n","    os.makedirs(RESULT_DIR)\n","\n","with open(save_dir, 'wb') as f:\n","    pickle.dump(results, f)"]},{"cell_type":"code","source":["df_total.loc[1613, 'text']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"w5a7GB2QQ_EZ","executionInfo":{"status":"ok","timestamp":1703948865666,"user_tz":-540,"elapsed":475,"user":{"displayName":"Trip Builder","userId":"18334374602196460472"}},"outputId":"8a385b07-7e43-4f5d-aba8-7346251710be"},"execution_count":155,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'촉촉하고 윤기가 있어 보여 좋아요 지인들에게 선물하고 싶어요'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":155}]},{"cell_type":"code","execution_count":147,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":90},"executionInfo":{"elapsed":572,"status":"ok","timestamp":1703948613627,"user":{"displayName":"Trip Builder","userId":"18334374602196460472"},"user_tz":-540},"id":"OdslgpdsRPOs","outputId":"cf26ca09-2bec-4bde-954f-db7107fcb08d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'사춘기 딸이 후기 보고 구입했는데 여드름에 효과 있어요 아이가 세안할 때 한번 누르기만 하면 거품이 풍부하게 나와서 시간도 단축되고 사용하기 편리해서 다른 제품보다 이 제품에 손이 가네요 헹굴 때도 미끌거리 지 않고 잘 헹궈져서 좋아요 단점은 소독할 때 나는 알콜향이 나 고 지성인 부분 외에 다른 피부를 건조하게 하네요 가격도 다른 여드름 상품 치고는 비싸지 않은 편이고  대용량이라 좋아요 붉은 여드름에도 진정 효과가 있지만 블랙헤드에는 효과가 없어요 눈가는 피해서 폼을 사용해야 해요'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":147}],"source":["df_train.loc[500, 'text']"]},{"cell_type":"code","source":["len(df_train.loc[500, 'text'].split(' '))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DMhaFz-sQKvu","executionInfo":{"status":"ok","timestamp":1703948636612,"user_tz":-540,"elapsed":538,"user":{"displayName":"Trip Builder","userId":"18334374602196460472"}},"outputId":"3a405cea-7b94-4fa8-df6c-7e127c833af8"},"execution_count":148,"outputs":[{"output_type":"execute_result","data":{"text/plain":["73"]},"metadata":{},"execution_count":148}]},{"cell_type":"code","source":["over = []\n","for i in range(len(df_total)):\n","  if len(df_total.loc[i, 'text'].split(' ')) > 70:\n","    over.append(i)\n","print(len(over))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HhiaaoT3QOnk","executionInfo":{"status":"ok","timestamp":1703948757965,"user_tz":-540,"elapsed":4,"user":{"displayName":"Trip Builder","userId":"18334374602196460472"}},"outputId":"15d58ba7-fb75-4ddc-9a43-02a8597546cf"},"execution_count":150,"outputs":[{"output_type":"stream","name":"stdout","text":["130\n"]}]},{"cell_type":"code","source":["over"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MRHjIx6-Qsn5","executionInfo":{"status":"ok","timestamp":1703948769298,"user_tz":-540,"elapsed":3,"user":{"displayName":"Trip Builder","userId":"18334374602196460472"}},"outputId":"068173f6-8c8e-46a2-946d-1ede9889959b"},"execution_count":151,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[130,\n"," 594,\n"," 944,\n"," 1185,\n"," 1385,\n"," 1405,\n"," 1504,\n"," 1719,\n"," 1882,\n"," 1908,\n"," 1911,\n"," 1920,\n"," 2212,\n"," 2584,\n"," 2641,\n"," 2682,\n"," 2701,\n"," 2915,\n"," 2924,\n"," 3040,\n"," 3171,\n"," 3289,\n"," 3354,\n"," 3641,\n"," 3681,\n"," 3791,\n"," 3806,\n"," 3845,\n"," 3890,\n"," 3892,\n"," 3902,\n"," 3964,\n"," 3969,\n"," 4040,\n"," 4097,\n"," 4099,\n"," 4140,\n"," 4234,\n"," 4271,\n"," 4275,\n"," 4294,\n"," 4543,\n"," 4564,\n"," 4589,\n"," 4620,\n"," 4640,\n"," 4670,\n"," 4707,\n"," 4719,\n"," 4775,\n"," 4900,\n"," 4916,\n"," 5101,\n"," 5183,\n"," 5195,\n"," 5196,\n"," 5203,\n"," 5213,\n"," 5324,\n"," 5329,\n"," 5346,\n"," 5369,\n"," 5432,\n"," 5443,\n"," 5495,\n"," 5545,\n"," 5945,\n"," 6211,\n"," 6525,\n"," 6545,\n"," 6552,\n"," 6591,\n"," 6643,\n"," 6887,\n"," 7169,\n"," 7226,\n"," 7277,\n"," 7278,\n"," 7330,\n"," 7343,\n"," 7363,\n"," 7371,\n"," 7404,\n"," 7523,\n"," 7531,\n"," 7583,\n"," 7649,\n"," 7747,\n"," 7751,\n"," 7768,\n"," 7832,\n"," 7964,\n"," 7974,\n"," 8292,\n"," 8349,\n"," 8371,\n"," 8411,\n"," 8471,\n"," 8593,\n"," 8619,\n"," 8750,\n"," 8859,\n"," 11681,\n"," 11720,\n"," 11764,\n"," 12176,\n"," 12493,\n"," 12539,\n"," 12561,\n"," 12565,\n"," 12567,\n"," 12601,\n"," 12620,\n"," 12652,\n"," 12654,\n"," 12656,\n"," 12678,\n"," 12712,\n"," 12729,\n"," 12748,\n"," 13115,\n"," 13144,\n"," 13936,\n"," 14035,\n"," 14118,\n"," 14227,\n"," 14264,\n"," 14506,\n"," 14509,\n"," 14989]"]},"metadata":{},"execution_count":151}]},{"cell_type":"code","source":["df_total.drop(over, inplace=True)\n","df_total.reset_index(inplace=True, drop=True)\n","df_total"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":579},"id":"GrxMin90QufT","executionInfo":{"status":"ok","timestamp":1703948809951,"user_tz":-540,"elapsed":4,"user":{"displayName":"Trip Builder","userId":"18334374602196460472"}},"outputId":"98282942-431e-44ff-e439-78ebb81294fe"},"execution_count":152,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                    text  \\\n","0                     유통기한도 넉넉하고 구성도 많아서 선물 하기 좋네요 만족합니다   \n","1                                    구성도 알차고 촉촉하고 너무 좋아용   \n","2      대용량으로 넉넉하게 사용할 수 있고 무난하고 순한 편이네요 제품 구성은 좋으나 가격...   \n","3             참존을 이 구성에 이 가격으로 사도 되나 싶은 생각이 드는 제품 양도 많아요   \n","4                                 끈적임 없이 잘 흡수되어 매우 만족합니다   \n","...                                                  ...   \n","14865  얼굴에 열이 나면 트러블이 생겨서 지인 소개로 구입했습니다 ㅇㅇㅇ 마스트가 알로에 ...   \n","14866  수분 부족 지성 피부입니다 사용해 봤는데 자극이 없어 좋습니다 팩하고 세안 했는데 ...   \n","14867  성분이 알로에라 촉촉함 하나는 기대했는데 촉촉하지가 않습니다 피부가 번들거리기만 해...   \n","14868  잘 쓰고 있네요 트러블이 요즘 잦아서 쓰는 중인데 확실히 트러블이 가라앉아서 좋네요...   \n","14869  시트지를 알로에로 만들었다고 해서 구매했습니다 생각보다 점성이 너무 있네요 제형이 ...   \n","\n","                                                  labels  \n","0      [B-POS, E-POS, B-POS, I-POS, I-POS, I-POS, E-P...  \n","1                    [B-POS, E-POS, B-POS, I-POS, E-POS]  \n","2      [B-POS, I-POS, I-POS, I-POS, E-POS, B-POS, I-P...  \n","3      [O, B-POS, I-POS, I-POS, I-POS, I-POS, I-POS, ...  \n","4             [B-POS, E-POS, B-POS, I-POS, I-POS, E-POS]  \n","...                                                  ...  \n","14865  [O, O, O, O, O, O, O, O, O, O, B-NEG, I-NEG, I...  \n","14866  [O, O, O, O, O, O, B-POS, I-POS, E-POS, O, O, ...  \n","14867  [B-NEG, I-NEG, I-NEG, I-NEG, I-NEG, B-NEG, E-N...  \n","14868  [O, O, O, O, O, O, O, O, O, B-POS, I-POS, E-PO...  \n","14869  [O, O, O, O, O, O, B-NEG, I-NEG, E-NEG, B-NEG,...  \n","\n","[14870 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-a0cb705a-f00d-41a7-9831-999e2b93b69f\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>유통기한도 넉넉하고 구성도 많아서 선물 하기 좋네요 만족합니다</td>\n","      <td>[B-POS, E-POS, B-POS, I-POS, I-POS, I-POS, E-P...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>구성도 알차고 촉촉하고 너무 좋아용</td>\n","      <td>[B-POS, E-POS, B-POS, I-POS, E-POS]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>대용량으로 넉넉하게 사용할 수 있고 무난하고 순한 편이네요 제품 구성은 좋으나 가격...</td>\n","      <td>[B-POS, I-POS, I-POS, I-POS, E-POS, B-POS, I-P...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>참존을 이 구성에 이 가격으로 사도 되나 싶은 생각이 드는 제품 양도 많아요</td>\n","      <td>[O, B-POS, I-POS, I-POS, I-POS, I-POS, I-POS, ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>끈적임 없이 잘 흡수되어 매우 만족합니다</td>\n","      <td>[B-POS, E-POS, B-POS, I-POS, I-POS, E-POS]</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>14865</th>\n","      <td>얼굴에 열이 나면 트러블이 생겨서 지인 소개로 구입했습니다 ㅇㅇㅇ 마스트가 알로에 ...</td>\n","      <td>[O, O, O, O, O, O, O, O, O, O, B-NEG, I-NEG, I...</td>\n","    </tr>\n","    <tr>\n","      <th>14866</th>\n","      <td>수분 부족 지성 피부입니다 사용해 봤는데 자극이 없어 좋습니다 팩하고 세안 했는데 ...</td>\n","      <td>[O, O, O, O, O, O, B-POS, I-POS, E-POS, O, O, ...</td>\n","    </tr>\n","    <tr>\n","      <th>14867</th>\n","      <td>성분이 알로에라 촉촉함 하나는 기대했는데 촉촉하지가 않습니다 피부가 번들거리기만 해...</td>\n","      <td>[B-NEG, I-NEG, I-NEG, I-NEG, I-NEG, B-NEG, E-N...</td>\n","    </tr>\n","    <tr>\n","      <th>14868</th>\n","      <td>잘 쓰고 있네요 트러블이 요즘 잦아서 쓰는 중인데 확실히 트러블이 가라앉아서 좋네요...</td>\n","      <td>[O, O, O, O, O, O, O, O, O, B-POS, I-POS, E-PO...</td>\n","    </tr>\n","    <tr>\n","      <th>14869</th>\n","      <td>시트지를 알로에로 만들었다고 해서 구매했습니다 생각보다 점성이 너무 있네요 제형이 ...</td>\n","      <td>[O, O, O, O, O, O, B-NEG, I-NEG, E-NEG, B-NEG,...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>14870 rows × 2 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a0cb705a-f00d-41a7-9831-999e2b93b69f')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-a0cb705a-f00d-41a7-9831-999e2b93b69f button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-a0cb705a-f00d-41a7-9831-999e2b93b69f');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-5ae72ae0-9678-4eb9-b453-1ec9075ce36d\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5ae72ae0-9678-4eb9-b453-1ec9075ce36d')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-5ae72ae0-9678-4eb9-b453-1ec9075ce36d button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_6b99af5b-e4f6-4f1a-9c16-a26a2a1cb966\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_total')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_6b99af5b-e4f6-4f1a-9c16-a26a2a1cb966 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df_total');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":152}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-XZxxhm4QyHf"},"outputs":[],"source":["df_train[df_train['text']=='보습이 잘 되네요 화장하니 얼굴에 광이나서 좋습니다 피부 결 모공도 개선 효과 있기를 기대해 봅니다']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_FJukuMiIDaI"},"outputs":[],"source":["df_train.loc[11999, 'labels']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1703946475319,"user":{"displayName":"Trip Builder","userId":"18334374602196460472"},"user_tz":-540},"id":"mGw8PQLxHz4L","outputId":"5a2ab485-3a10-43ba-acd9-6028ce8f2d46"},"outputs":[{"data":{"text/plain":["15"]},"execution_count":73,"metadata":{},"output_type":"execute_result"}],"source":["len(df_train.loc[11999, 'text'].split(' '))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1703946486816,"user":{"displayName":"Trip Builder","userId":"18334374602196460472"},"user_tz":-540},"id":"0ZD_iqokH-bo","outputId":"0a40a73e-aea2-4887-e285-ac671d12a47c"},"outputs":[{"data":{"text/plain":["15"]},"execution_count":74,"metadata":{},"output_type":"execute_result"}],"source":["len(df_train.loc[11999, 'labels'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N_ec-88nQ6iQ"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"elapsed":286,"status":"ok","timestamp":1703932097361,"user":{"displayName":"Trip Builder","userId":"18334374602196460472"},"user_tz":-540},"id":"RLZ8XnUERCki","outputId":"9dff9454-fe67-41c0-e177-981a9f2ca383"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-b8020d66-1b8b-41c4-9b2e-561eaa66c221\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b8020d66-1b8b-41c4-9b2e-561eaa66c221')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-b8020d66-1b8b-41c4-9b2e-561eaa66c221 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-b8020d66-1b8b-41c4-9b2e-561eaa66c221');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    </div>\n","  </div>\n"],"text/plain":["Empty DataFrame\n","Columns: [text, labels]\n","Index: []"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["df_train[df_train['text']=='보습이 잘 되네요 화장하니 얼굴에 광이나서 좋습니다 피부 결 모공도 개선 효과 있기를 기대해 봅니다']"]},{"cell_type":"markdown","metadata":{"id":"cjlm4CT70aIj"},"source":["# 참고"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3EYqq8f60bYV"},"outputs":[],"source":["def ot2bieos_ts(ts_tag_sequence): # ts_tag_sequence: # ['O', 'O', 'T-NEG', 'O', 'O', 'O', 'O', 'O', 'O']\n","    n_tags = len(ts_tag_sequence) # n_tags: 9\n","    new_ts_sequence = []\n","    prev_pos = '$$$'\n","\n","    for i in range(n_tags):\n","        cur_ts_tag = ts_tag_sequence[i]\n","        if cur_ts_tag == 'O' or cur_ts_tag == 'EQ':\n","            new_ts_sequence.append('O')\n","            cur_pos = 'O'\n","\n","        else: # 'T-NEG'\n","            cur_pos, cur_sentiment = cur_ts_tag = cur_ts_tag.split('-')\n","            # cur_ts_tag: ['T', 'NEG']\n","            # cur_pos: T\n","            # cur_sentiment: NEG\n","\n","            if cur_pos != prev_pos: # 현재 위치\n","\n","                if i == n_tags -1: # 마지막 태그이면\n","                    new_ts_sequence.append('S-%s' % cur_sentiment) # Single tag\n","\n","                else: # 마지막 태그가 아니면\n","                    next_ts_tag = ts_tag_sequence[i + 1] # 다음 태그 추출\n","                    if next_ts_tag == 'O': # 다음 태그가 O이면 Single tag\n","                        new_ts_sequence.append('S-%s' % cur_sentiment)\n","\n","                    else: # 다음 태그가 O이 아니면 Beginning tag\n","                        new_ts_sequence.append('B-%s' % cur_sentiment)\n","\n","            else: # cur_pos == prev_pos\n","                if i == n_tags -1: # 마지막 태그이면 Ending tag\n","                    new_ts_sequence.append('E-%s' % cur_sentiment)\n","\n","                else: # 마지막 태그가 아니면\n","                    next_ts_tag = ts_tag_sequence[i + 1] # 다음 태그 추출\n","                    if next_ts_tag == 'O': # 다음 태그가 O이면 Ending tag\n","                        new_ts_sequence.append('E-%s' % cur_sentiment)\n","                    else: # 다음 태그가 O이 아니면 Inside tag\n","                        new_ts_sequence.append('I-%s' % cur_sentiment)\n","        prev_pos = cur_pos\n","    return new_ts_sequence\n","\n","def tag2ts(ts_tag_sequence): # ts_tag_sequence: [O, O, S-NEG, O, O, O, O, O, O]\n","    n_tags = len(ts_tag_sequence) # 9\n","    ts_sequence, sentiments = [], []\n","    beg, end = -1, -1\n","\n","    for i in range(n_tags):\n","        ts_tag = ts_tag_sequence[i]\n","        eles = ts_tag.split('-') # ['O'] 혹은 ['S', 'NEG']\n","\n","        if len(eles) == 2: # 태그가 O이 아니면\n","            pos, sentiment = eles\n","            # pos: S\n","            # sentiment: NEG\n","        else: # 태그가 O이면\n","            pos, sentiment = 'O', 'O'\n","\n","        if sentiment != 'O': # 태그가 O이 아니면, sentiment: NEG\n","            sentiments.append(sentiment) # sentiment = ['NEG']\n","\n","        if pos == 'S': # Single tag이면\n","            ts_sequence.append((i, i, sentiment)) # 튜플 추가, [(2, 2, 'NEG')]\n","            sentiments = [] # 초기화\n","\n","        elif pos == 'B': # Beginning tag이면\n","            beg = i\n","            if len(sentiments) > 1: # 앞의 태그가 O이 여러개였다면 ['O', ..., 'O'] 이렇게 붙었을 것\n","                sentiments = [sentiments[-1]] # 마지막 한 개만 가져옴\n","\n","        elif pos == 'E': # Ending tag이면\n","            end = i\n","\n","            if end > begin > -1 and len(set(sentiments)) == 1:\n","                ts_sequence.append((begin, end, sentiment)) # 위치표시, [(3, 5, 'NEG')]\n","                sentiments = []\n","                begin, end = -1, -1\n","\n","    return ts_sequence # [(2, 2, 'NEG'), (3, 5, 'NEG')]\n","\n","class InputExample:\n","    def __init__(self, guid, text_a, text_b=None, label=None):\n","        '''\n","        Inputs:\n","            guid: Unique id for the example.\n","            text_a: string. The untokenized text of the first sequence. For single\n","                    sequence tasks, only this sequence must be specified.\n","            text_b: (Optional) string. The untokenized text of the second sequence.\n","            label: (Optional) string. The label of the example. This should be specified for train and dev examples.\n","        '''\n","        self.guid = guid\n","        self.text_a = text_a\n","        self.text_b = text_b\n","        self.label = label\n","\n","class ABSAProcessor:\n","    def get_examples(self, data_dir, set_type='train'):\n","        return self._create_examples(data_dir=data_dir, set_type=set_type)\n","\n","    def get_labels(self):\n","        return ['O', 'EQ', 'B-POS', 'I-POS', 'E-POS', 'S-POS',\n","                 'B-NEG', 'I-NEG', 'E-NEG', 'S-NEG',\n","                 'B-NEU', 'I-NEU', 'E-NEU', 'S-NEU']\n","\n","    def _create_examples(self, data_dir, set_type):\n","        examples = []\n","        file = os.path.join(data_dir, '%s.txt' % set_type) # train.txt, test.txt 파일 불러옴\n","        class_count = np.zeros(3) # 긍정, 부정, 중립 개수 세기 위해\n","        with open(file, 'r', encoding='UTF-8') as fp:\n","            sample_id = 0\n","            for line in fp:\n","                '''\n","                sent_string: But the staff was so horrible to us.\n","                tag_string: But=O the=O staff=T-NEG was=O so=O horrible=O to=O us=O .=O\n","\n","                words: [But, the, staff, was, so, horrible, to, us, .]\n","                tags: [O, O, S-NEG, O, O, O, O, O, O]\n","                '''\n","                sent_string, tag_string = line.strip().split('####') # strip() 양쪽 끝에 공백 제거\n","                # sent_stirng: But the staff was so horrible to us.\n","                # tag_string: But=O the=O staff=T-NEG was=O so=O horrible=O to=O us=O .=O\n","\n","                words, tags =[], []\n","                for tag_item in tag_string.split(' '): # tag_item: ['But=O', 'the=O', ...]\n","                    eles = tag_item.split('=') # ['But', 'O']\n","                    if len(eles) == 1:\n","                        raise Exception('Invalid samples %s...' % tag_string) # 오류\n","\n","                    elif len(eles) == 2:\n","                        word, tag = eles\n","                        # word: But\n","                        # tag: O\n","\n","                    else:\n","                        word = ''.join((len(eles) - 2) * ['=']) # else에서 2 뺀 만큼의 =를 word로 지정\n","                        tag = eles[-1] # else에서 마지막 원소를 tag로 지정\n","\n","                    words.append(word) # [But, the, staff, was, so, horrible, to, us, .]\n","                    tags.append(tag) # ['O', 'O', 'T-NEG', 'O', 'O', 'O', 'O', 'O', 'O'], Semuel 태그\n","\n","                tags = ot2bieos_ts(tags) # ['O', 'O', 'S-NEG', 'O', 'O', 'O', 'O', 'O', 'O'], 논문 태그로 바꿈\n","\n","                guid = '%s-%s' % (set_type, sample_id) # train-0\n","                text_a = ' '.join(words) # But the staff was so horrible to us .\n","                gold_ts = tag2ts(ts_tag_sequence=tags) # [(2, 2, 'NEG'), (3, 5, 'NEG')]\n","                for (_, _, s) in gold_ts:\n","                    if s == 'POS':\n","                        class_count[0] += 1\n","\n","                    if s == 'NEG':\n","                        class_count[1] += 1\n","\n","                    if s == 'NEU':\n","                        class_count[2] += 1\n","\n","                examples.append(InputExample(guid=guid, text_a=text_a, text_b=None, label=tags))\n","                sample_id += 1\n","\n","\n","            print('%s class count: %s' % (set_type, class_count))\n","            print(f'review length: {sample_id:,}\\t total class count: {int(sum(class_count)):,}')\n","            return examples\n","\n","#===============================================================================\n","\n","class SeqInputFeatures:\n","    def __init__(self, input_ids, input_mask, segment_ids, label_ids, evaluate_label_ids):\n","        self.input_ids = input_ids\n","        self.input_mask = input_mask\n","        self.segment_ids = segment_ids\n","        self.label_ids = label_ids\n","        self.evaluate_label_ids = evaluate_label_ids\n","\n","def convert_examples_to_seq_features(examples, label_list, tokenizer,\n","                                     cls_token='[CLS]', sep_token='[SEP]', pad_token=0, sequence_a_segment_id=0,\n","                                     cls_token_segment_id=1, pad_token_segment_id=0, mask_padding_with_zero=True):\n","\n","    label_map = {label:i for i, label in enumerate(label_list)} # {'O':0, 'EQ':1, 'B-POS':2, 'I-POS':3, ...\n","    features = [] # 초기화\n","    max_seq_length = -1 # 초기화\n","    examples_tokenized = [] # 초기화\n","    for example in examples:\n","        tokens_a, labels_a = [], [] # 초기화\n","        evaluate_label_ids = [] # 초기화\n","        words = example.text_a.split(' ') # [But, the, staff, was, so, horrible, to, us, .]\n","        wid, tid = 0, 0\n","        for word, label in zip(words, example.label): # But O\n","            subwords = tokenizer.tokenize(word) # ! 단어별 토근화\n","            tokens_a.extend(subwords) # 토큰화한 문장\n","            if label != 'O':\n","                labels_a.extend([label] + ['EQ'] * (len(subwords) - 1)) # [B-POS, EQ, EQ]\n","            else: # 라벨이 O이면\n","                labels_a.extend(['O'] * len(subwords)) # subwords 개수만큼 O 삽입\n","            evaluate_label_ids.append(tid) # [0]\n","            wid += 1\n","            tid += len(subwords)\n","\n","        assert tid == len(tokens_a) # subwords의 개수만큼 계속 더했기 때문에, 현재까지 토큰화된 단어의 수와 같을 것\n","        # 참이면 계속 진행, 아니면 에러 띄움\n","\n","        evaluate_label_ids = np.array(evaluate_label_ids, dtype=np.int32) # 리스트에서 어레이로 바뀜, [0]\n","        examples_tokenized.append((tokens_a, labels_a, evaluate_label_ids)) # [(['But'], ['O'], [0]), (['the'], ['O'], [0]), ...]\n","        if len(tokens_a) > max_seq_length:\n","            max_seq_length = len(tokens_a) # sequence 길이\n","\n","    max_seq_length += 2\n","    for (tokens_a, labels_a, evaluate_label_ids) in examples_tokenized: # (['But'], ['O'], [0])\n","        tokens = tokens_a + [sep_token] # ['But', '[SEP]']\n","        segment_ids = [sequence_a_segment_id] * len(tokens) # [0, 0]\n","        labels = labels_a + ['O'] # sep_token이 더해졌기 때문에 'O' 하나 더 추가, ['O', 'O']\n","\n","        # if cls_token_at_end, CLS 토큰을 끝에 둘 것임\n","        tokens = [cls_token] + tokens # ['[CLS]', 'But', '[SEP]']\n","        segment_ids = [cls_token_segment_id] + segment_ids # [1, 0, 0]\n","        labels = ['O'] + labels # ['O', 'O', 'O']\n","        evaluate_label_ids += 1 # [1]\n","\n","        input_ids = tokenizer.convert_tokens_to_ids(tokens) # 정수 인코딩, [ 101, 2021, 1996, 3095, 2001, 2061, 9202, 2000, 2149, 1012,  102, ...]\n","        input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids) # padding 아닌 부분을 1로 채움\n","        padding_length = max_seq_length - len(input_ids) # padding 할 길이 구함\n","        label_ids = [label_map[label] for label in labels] # labels를 정수 인코딩함, [0, 0, 9, 0, 0, 0, 0, 0, 0]\n","\n","        # if pad_on_left, padding을 왼쪽에다 할 것임\n","        input_ids = input_ids + ([pad_token] * padding_length) # [ 101, 2021, 1996, 3095, 2001, 2061, 9202, 2000, 2149, 1012,  102,    0, ...]\n","        input_mask = input_mask + ([0 if mask_padding_with_zero else 1] * padding_length) # [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]\n","        segment_ids = segment_ids + ([pad_token_segment_id] * padding_length) # [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]\n","\n","        label_ids = label_ids + ([0] * padding_length) # [0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]\n","\n","        assert len(input_ids) == max_seq_length\n","        assert len(input_mask) == max_seq_length\n","        assert len(segment_ids) == max_seq_length\n","        assert len(label_ids) == max_seq_length # 길이가 다 맞으면 계속함\n","\n","        features.append(SeqInputFeatures(input_ids=input_ids,\n","                                         input_mask=input_mask,\n","                                         segment_ids=segment_ids,\n","                                         label_ids=label_ids,\n","                                         evaluate_label_ids=evaluate_label_ids))\n","    print('maximal sequence length is %d' % (max_seq_length))\n","    return features\n","\n","def get_absa_loader(args, tokenizer, mode):\n","    '''\n","    Input:\n","        args: configuration\n","        tokenizer: tokenizer (Bert-Tokenizer)\n","\n","    Returns:\n","        dataset: dataset consist of input_ids, input_mask, segment_ids, label_ids according to dataset\n","        all_evaluate_label_ids: label_ids (such as E-POS, I-POS, S-NEG, etc..)\n","\n","    examples:\n","        (tensor([ 101, 2021, 1996, 3095, 2001, 2061, 9202, 2000, 2149, 1012,  102,    0,\n","                0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","                0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","                0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","                0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","                0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","                0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","                0,    0,    0,    0,    0]),\n","\n","         tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n","\n","         tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n","\n","         tensor([0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n","    '''\n","\n","    processor = ABSAProcessor()\n","\n","    # cache' name\n","    cached_features_file = os.path.join(args.semeval_dir,\n","                                        'cached_{}_{}_{}'.format(\n","                                        mode,\n","                                        list(filter(None, args.model_name_or_path.split('/'))).pop(),\n","                                        str(args.max_seq_length)))\n","\n","\n","    if os.path.exists(cached_features_file):\n","        print('cached_features_file:', cached_features_file)\n","        features = torch.load(cached_features_file)\n","\n","    else:\n","        label_list = processor.get_labels()\n","        examples = processor.get_examples(args.semeval_dir, mode)\n","\n","        features = convert_examples_to_seq_features(examples=examples, label_list=label_list, tokenizer=tokenizer,\n","                                                    cls_token_segment_id=0, pad_token_segment_id=0)\n","\n","        torch.save(features, cached_features_file) # 파일로 저장 ===========* 여기까지 구현 목표\n","\n","\n","    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n","    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n","    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n","\n","    all_label_ids = torch.tensor([f.label_ids for f in features], dtype=torch.long)\n","    all_evaluate_label_ids = [f.evaluate_label_ids for f in features]\n","    dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n","\n","    if mode=='train':\n","        sampler = RandomSampler(dataset)\n","    elif mode=='valid' or mode=='test':\n","        sampler = SequentialSampler(dataset)\n","\n","    dataloader = DataLoader(dataset, sampler=sampler, batch_size=args.batch_size)\n","\n","    return (dataloader, all_evaluate_label_ids)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5qZ0ZlIl5cnH"},"outputs":[],"source":["class BertABSATagger(BertPreTrainedModel):\n","    def __init__(self, bert_config):\n","        \"\"\"\n","\n","        :param bert_config: configuration for bert model\n","        \"\"\"\n","        super(BertABSATagger, self).__init__(bert_config)\n","        self.num_labels = bert_config.num_labels\n","        self.tagger_config = TaggerConfig()\n","        self.tagger_config.absa_type = bert_config.absa_type.lower()\n","        if bert_config.tfm_mode == 'finetune':\n","            # initialized with pre-trained BERT and perform finetuning\n","            # print(\"Fine-tuning the pre-trained BERT...\")\n","            self.bert = BertModel(bert_config)\n","        else:\n","            raise Exception(\"Invalid transformer mode %s!!!\" % bert_config.tfm_mode)\n","        self.bert_dropout = nn.Dropout(bert_config.hidden_dropout_prob)\n","        # fix the parameters in BERT and regard it as feature extractor\n","        if bert_config.fix_tfm:\n","            # fix the parameters of the (pre-trained or randomly initialized) transformers during fine-tuning\n","            for p in self.bert.parameters():\n","                p.requires_grad = False\n","\n","        self.tagger = None\n","        if self.tagger_config.absa_type == 'linear':\n","            # hidden size at the penultimate layer\n","            penultimate_hidden_size = bert_config.hidden_size\n","        else:\n","            self.tagger_dropout = nn.Dropout(self.tagger_config.hidden_dropout_prob)\n","            if self.tagger_config.absa_type == 'lstm':\n","                self.tagger = LSTM(input_size=bert_config.hidden_size,\n","                                   hidden_size=self.tagger_config.hidden_size,\n","                                   bidirectional=self.tagger_config.bidirectional)\n","            elif self.tagger_config.absa_type == 'gru':\n","                self.tagger = GRU(input_size=bert_config.hidden_size,\n","                                  hidden_size=self.tagger_config.hidden_size,\n","                                  bidirectional=self.tagger_config.bidirectional)\n","            elif self.tagger_config.absa_type == 'tfm':\n","                # transformer encoder layer\n","                self.tagger = nn.TransformerEncoderLayer(d_model=bert_config.hidden_size,\n","                                                         nhead=12,\n","                                                         dim_feedforward=4*bert_config.hidden_size,\n","                                                         dropout=0.1)\n","            elif self.tagger_config.absa_type == 'san':\n","                # vanilla self attention networks\n","                self.tagger = SAN(d_model=bert_config.hidden_size, nhead=12, dropout=0.1)\n","            elif self.tagger_config.absa_type == 'crf':\n","                self.tagger = CRF(num_tags=self.num_labels)\n","            else:\n","                raise Exception('Unimplemented downstream tagger %s...' % self.tagger_config.absa_type)\n","            penultimate_hidden_size = self.tagger_config.hidden_size\n","        self.classifier = nn.Linear(penultimate_hidden_size, bert_config.num_labels)\n","\n","    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None,\n","                position_ids=None, head_mask=None):\n","        outputs = self.bert(input_ids, position_ids=position_ids, token_type_ids=token_type_ids,\n","                            attention_mask=attention_mask, head_mask=head_mask)\n","        # the hidden states of the last Bert Layer, shape: (bsz, seq_len, hsz)\n","        tagger_input = outputs[0]\n","        tagger_input = self.bert_dropout(tagger_input)\n","        #print(\"tagger_input.shape:\", tagger_input.shape)\n","        if self.tagger is None or self.tagger_config.absa_type == 'crf':\n","            # regard classifier as the tagger\n","            logits = self.classifier(tagger_input)\n","        else:\n","            if self.tagger_config.absa_type == 'lstm':\n","                # customized LSTM\n","                classifier_input, _ = self.tagger(tagger_input)\n","            elif self.tagger_config.absa_type == 'gru':\n","                # customized GRU\n","                classifier_input, _ = self.tagger(tagger_input)\n","            elif self.tagger_config.absa_type == 'san' or self.tagger_config.absa_type == 'tfm':\n","                # vanilla self-attention networks or transformer\n","                # adapt the input format for the transformer or self attention networks\n","                tagger_input = tagger_input.transpose(0, 1)\n","                classifier_input = self.tagger(tagger_input)\n","                classifier_input = classifier_input.transpose(0, 1)\n","            else:\n","                raise Exception(\"Unimplemented downstream tagger %s...\" % self.tagger_config.absa_type)\n","            classifier_input = self.tagger_dropout(classifier_input)\n","            logits = self.classifier(classifier_input)\n","        outputs = (logits,) + outputs[2:]\n","\n","        if labels is not None:\n","            if self.tagger_config.absa_type != 'crf':\n","                loss_fct = CrossEntropyLoss()\n","                if attention_mask is not None:\n","                    active_loss = attention_mask.view(-1) == 1\n","                    active_logits = logits.view(-1, self.num_labels)[active_loss]\n","                    active_labels = labels.view(-1)[active_loss]\n","                    loss = loss_fct(active_logits, active_labels)\n","                else:\n","                    loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","                outputs = (loss,) + outputs\n","            else:\n","                log_likelihood = self.tagger(inputs=logits, tags=labels, mask=attention_mask)\n","                loss = -log_likelihood\n","                outputs = (loss,) + outputs\n","        return outputs\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gU3WQ2fELXZk"},"outputs":[],"source":["# coding=utf-8\n","# Copyright 2018 Google AI Language, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.\n","# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","\n","from transformers import PreTrainedModel, BertModel, BertConfig\n","from torch.utils.data import Dataset\n","\n","import torch\n","from transformers import BertModel\n","from torch import nn\n","\n","class BertLayerNorm(nn.Module):\n","    def __init__(self, hidden_size, eps=1e-12):\n","        super(BertLayerNorm, self).__init__()\n","        self.weight = nn.Parameter(torch.ones(hidden_size))\n","        self.bias = nn.Parameter(torch.zeros(hidden_size))\n","        self.variance_epsilon = eps\n","\n","    def forward(self, x):\n","        u = x.mean(-1, keepdim=True)\n","        s = (x - u).pow(2).mean(-1, keepdim=True)\n","        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n","        return self.weight * x + self.bias\n","\n","class BertPreTrainedModel(PreTrainedModel):\n","    config_class = BertConfig\n","    base_model_prefix = \"bert\"\n","\n","    def __init__(self, *inputs, **kwargs):\n","        super(BertPreTrainedModel, self).__init__(*inputs, **kwargs)\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, (nn.Linear, nn.Embedding)):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","        elif isinstance(module, BertLayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","        if isinstance(module, nn.Linear) and module.bias is not None:\n","            module.bias.data.zero_()\n","\n","###########################################################################################################################\n","\n","class TaggerConfig:\n","    def __init__(self):\n","        self.hidden_dropout_prob = 0.1\n","        self.hidden_size = 768\n","\n","class SAN(nn.Module):\n","    def __init__(self, model, nhead, dropout=0.1):\n","        super(SAN, self).__init__()\n","        self.model = model\n","        self.self_attn = nn.MultiheadAttention(model, nhead, dropout=dropout)\n","        self.dropout = nn.Dropout(p=dropout)\n","        self.norm = nn.LayerNorm(model)\n","\n","    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n","        src2, _ = self.self_attn(src, src, src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask) # (key, query, value)\n","        src = src + self.dropout(src2)\n","        src = self.norm(src)\n","        return src\n","\n","class BertABSATagger(BertPreTrainedModel):\n","    def __init__(self, bert_config):\n","        super(BertABSATagger, self).__init__(bert_config)\n","        self.num_labels = bert_config.num_labels\n","        self.tagger_config = TaggerConfig()\n","        self.bert = BertModel(bert_config)\n","        self.bert_dropout = nn.Dropout(bert_config.hidden_dropout_prob)\n","        if bert_config.fix_tfm:\n","            for p in self.bert.parameters():\n","                p.required_grad = False  # Frizen\n","\n","        self.tagger_dropout = nn.Dropout(self.tagger_config.hidden_dropout_prob)\n","        self.tagger = SAN(model=bert_config.hidden_size, nhead=12, dropout=0.1)\n","        penultimate_hidden_size = self.tagger_config.hidden_size\n","        self.classifier = nn.Linear(penultimate_hidden_size, self.num_labels)\n","\n","    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None, position_ids=None, head_mask=None):\n","        outputs = self.bert(\n","            input_ids,\n","            position_ids=position_ids,\n","            token_type_ids=token_type_ids,\n","            attention_mask=attention_mask,\n","            head_mask=head_mask\n","        )\n","\n","        tagger_input = outputs[0] # pooler\n","        tagger_input = self.bert_dropout(tagger_input)\n","        tagger_input = tagger_input.transpose(0, 1) # 각 성분에 대해서 classification을 하기 위함.\n","        classifier_input = self.tagger(tagger_input)\n","        classifier_input = classifier_input.transpose(0, 1)\n","        classifier_input = self.tagger_dropout(classifier_input)\n","        logits = self.classifier(classifier_input)\n","\n","        outputs = (logits, ) + outputs[2:]\n","\n","        if labels is not None:\n","            criterion = nn.CrossEntropyLoss()\n","\n","            if attention_mask is not None:\n","                active_loss = attention_mask.view(-1) == 1\n","                active_logits = logits.view(-1, self.num_labels)[active_loss]\n","                active_labels = labels.view(-1)[active_loss]\n","                loss = criterion(active_logits, active_labels)\n","\n","            else:\n","                loss = criterion(logits.view(-1, self.num_labels), labels.view(-1))\n","\n","            outputs = (loss, ) + outputs\n","\n","        return outputs\n","\n","\n","class ABSADataset(Dataset):\n","    def __init__(self, args, dataframe, tokenizer):\n","        self.tokenizer = tokenizer\n","        self.data = dataframe\n","        self.reviews = dataframe.text\n","        self.labels = dataframe.stars\n","        self.max_seq_length = args.max_seq_length\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","\n","        review = self.reviews[idx]\n","\n","        inputs = self.tokenizer.encode_plus(\n","            review,\n","            None,\n","            add_special_tokens=True,\n","            max_length=self.max_seq_length,\n","            padding='max_length',\n","            return_token_type_ids=True,\n","            truncation=True\n","        )\n","\n","        input_ids = inputs['input_ids']\n","        masks = inputs['attention_mask']\n","        token_type_ids = inputs['token_type_ids']\n","\n","        return (\n","            torch.tensor(input_ids, dtype=torch.long), # token_ids\n","            torch.tensor(masks, dtype=torch.long), # attention_mask\n","            torch.tensor(token_type_ids, dtype=torch.long), # token_type_ids\n","            torch.tensor(self.labels[idx], dtype = float) # labels\n","        )"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["cjlm4CT70aIj"],"provenance":[],"mount_file_id":"132_UxT7pbizLrXxx8T4lSVvEKX-IOkWQ","authorship_tag":"ABX9TyN5FZ10+5WAFR5cn/U1lXTB"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"2b9cf905f6c442c8b67972d535060edb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b7daeae51a034a828def5236ed379dde","IPY_MODEL_4237d488d4cd4b7a988b57f5523b448f","IPY_MODEL_cefdb53b2d4948eebdcedbb000e5cedd"],"layout":"IPY_MODEL_776edeba834545749420e93a96ab5c72"}},"b7daeae51a034a828def5236ed379dde":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_783e10b81de8409a8134449330971fab","placeholder":"​","style":"IPY_MODEL_51a864486f42485990789e71683d5ebe","value":"model.safetensors: 100%"}},"4237d488d4cd4b7a988b57f5523b448f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_87aad134ae164f509d63dcefa07f9ecf","max":445000316,"min":0,"orientation":"horizontal","style":"IPY_MODEL_22a6d4c422d3416ab52b5e066a3acace","value":445000316}},"cefdb53b2d4948eebdcedbb000e5cedd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_948bc41d418243f19572ebf63b3f8c82","placeholder":"​","style":"IPY_MODEL_cb7410962b9c49aea3f84d56523f6861","value":" 445M/445M [00:03&lt;00:00, 113MB/s]"}},"776edeba834545749420e93a96ab5c72":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"783e10b81de8409a8134449330971fab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51a864486f42485990789e71683d5ebe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"87aad134ae164f509d63dcefa07f9ecf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22a6d4c422d3416ab52b5e066a3acace":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"948bc41d418243f19572ebf63b3f8c82":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb7410962b9c49aea3f84d56523f6861":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}